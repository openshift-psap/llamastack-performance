apiVersion: v1
kind: ConfigMap
metadata:
  name: locustfiles
  namespace: tekton-llamastack
data:
  locustfile_main.py: |
    """
    Main entry point for Locust tests.
    All configuration is done via environment variables.

    Usage:
        locust -f locustfile_main.py --host http://llamastack:8321 --headless
        
    Environment Variables:
        USERS              - Number of concurrent users (default: 10)
        SPAWN_RATE         - Users to spawn per second (default: 1)  
        RUN_TIME_SECONDS   - Test duration in seconds (default: 60)
        MCP_SERVER         - MCP server URL (default: in-cluster)
        MODEL              - Model name (default: vllm-inference/llama-32-3b-instruct)
        PROMPT             - Test prompt (default: "What is Kubernetes?")
        LOAD_SHAPE         - Shape to use: steady (default: steady)
        MLFLOW_URL         - MLflow tracking server URL (optional)
        MLFLOW_EXPERIMENT  - MLflow experiment name (default: llamastack-benchmarks)
    """
    import os
    import sys

    # Add current directory to path for imports
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, current_dir)

    # Import user classes - Locust auto-discovers HttpUser subclasses
    from locustfile_users import ResponsesMCPUser

    # Import hooks - registers event listeners as side effect
    from hooks import mlflow_hooks

    # Import shape based on LOAD_SHAPE env var
    shape_name = os.environ.get("LOAD_SHAPE", "steady")

    if shape_name == "steady":
        from shapes.steady import SteadyShape
    elif shape_name == "spike":
        from shapes.spike import SpikeShape
    elif shape_name == "realistic":
        from shapes.realistic import RealisticShape
    elif shape_name == "custom":
        from shapes.custom import CustomShape
    else:
        print(f"WARNING: Unknown shape '{shape_name}', using steady")
        from shapes.steady import SteadyShape

  locustfile_users.py: |
    """
    User classes for LlamaStack performance testing.
    Each user class represents a different type of API consumer.
    """
    import os
    import json
    from locust import HttpUser, task, between


    class ResponsesMCPUser(HttpUser):
        """
        User that calls the Responses API with MCP tool calling.
        This simulates a client using LlamaStack's agentic capabilities.
        """
        wait_time = between(1, 3)
        
        def on_start(self):
            """Called when a user starts - setup configuration."""
            self.mcp_server = os.environ.get("MCP_SERVER", "http://sdg-docs-mcp-server.llamastack.svc.cluster.local:8000/sse")
            self.model = os.environ.get("MODEL", "vllm-inference/llama-32-3b-instruct")
            self.prompt = os.environ.get("PROMPT", "What is Kubernetes?")
            
        @task
        def call_responses_with_mcp(self):
            """Call Responses API with MCP tool."""
            payload = {
                "model": self.model,
                "input": self.prompt,
                "tools": [{
                    "type": "mcp",
                    "server_label": "deepwiki",
                    "server_url": self.mcp_server,
                    "require_approval": "never"
                }]
            }
            
            with self.client.post(
                "/v1/responses",
                json=payload,
                name="responses-mcp",
                catch_response=True
            ) as response:
                if response.status_code == 200:
                    try:
                        data = response.json()
                        if "output" in data or "choices" in data:
                            response.success()
                        else:
                            response.failure(f"Unexpected response format: {list(data.keys())}")
                    except json.JSONDecodeError:
                        response.failure("Invalid JSON response")
                else:
                    response.failure(f"HTTP {response.status_code}: {response.text[:200]}")

  shapes_init.py: |
    # Load shapes package

  shapes_steady.py: |
    """
    Steady load shape - maintains constant user count.
    """
    import os
    from locust import LoadTestShape


    class SteadyShape(LoadTestShape):
        """
        Constant load shape - maintains steady user count for duration.
        Spawn rate defaults to user count so all users start immediately.
        
        Configuration via environment variables:
            USERS: Number of concurrent users (default: 10)
            SPAWN_RATE: Users to spawn per second (defaults to USERS)
            RUN_TIME_SECONDS: Test duration in seconds (default: 60)
        """
        
        def tick(self):
            """
            Called ~1/second. Return (users, spawn_rate) or None to stop.
            """
            users = int(os.environ.get("USERS", "10"))
            spawn_rate = users  # All users spawn instantly for flat load
            run_time = int(os.environ.get("RUN_TIME_SECONDS", "60"))
            
            run_time_elapsed = self.get_run_time()
            
            if run_time_elapsed > run_time:
                return None  # Stop the test
                
            return (users, spawn_rate)

  shapes_spike.py: |
    """
    Spike load shape - sudden spike to stress-test the system.

    Pattern:
      1. Baseline: low steady load (warm-up)
      2. Spike: sudden jump to high user count
      3. Hold: sustain the spike
      4. Drop: back to baseline

    Configuration via environment variables:
        SPIKE_BASELINE_USERS:    Users during baseline (default: 5)
        SPIKE_PEAK_USERS:        Users during spike (default: 100)
        SPIKE_BASELINE_DURATION: Seconds at baseline before spike (default: 30)
        SPIKE_RAMP_DURATION:     Seconds to ramp up to peak (default: 10)
        SPIKE_HOLD_DURATION:     Seconds to hold at peak (default: 60)
        SPIKE_COOLDOWN_DURATION: Seconds to cool down (default: 30)
        SPAWN_RATE:              Users per second for non-ramp phases (default: 10)

    Note: The ramp spawn rate is auto-calculated to guarantee peak users
    are reached within SPIKE_RAMP_DURATION.
    """
    import os
    import math
    from locust import LoadTestShape


    class SpikeShape(LoadTestShape):
        """
        Sudden spike pattern - baseline -> rapid spike -> hold -> drop.
        Ramp spawn rate is auto-calculated to guarantee the spike completes in time.
        """

        def __init__(self):
            super().__init__()

            self.baseline_users = int(os.environ.get("SPIKE_BASELINE_USERS", "5"))
            self.peak_users = int(os.environ.get("SPIKE_PEAK_USERS", "100"))
            self.spawn_rate = int(os.environ.get("SPAWN_RATE", "10"))

            self.baseline_duration = int(os.environ.get("SPIKE_BASELINE_DURATION", "30"))
            self.ramp_duration = int(os.environ.get("SPIKE_RAMP_DURATION", "10"))
            self.hold_duration = int(os.environ.get("SPIKE_HOLD_DURATION", "60"))
            self.cooldown_duration = int(os.environ.get("SPIKE_COOLDOWN_DURATION", "30"))

            # Auto-calculate ramp spawn rate to guarantee peak is reached in time
            users_to_spawn = self.peak_users - self.baseline_users
            if self.ramp_duration > 0 and users_to_spawn > 0:
                self.ramp_spawn_rate = max(1, math.ceil(users_to_spawn / self.ramp_duration))
            else:
                self.ramp_spawn_rate = self.spawn_rate

            # Phase boundaries (cumulative)
            self.t_baseline_end = self.baseline_duration
            self.t_ramp_end = self.t_baseline_end + self.ramp_duration
            self.t_hold_end = self.t_ramp_end + self.hold_duration
            self.t_cooldown_end = self.t_hold_end + self.cooldown_duration

            # Log the plan so operator knows exactly what will happen
            print(f"SpikeShape plan:")
            print(f"  Phase 1 [0s-{self.t_baseline_end}s]:     Baseline at {self.baseline_users} users (spawn_rate={self.spawn_rate}/s)")
            print(f"  Phase 2 [{self.t_baseline_end}s-{self.t_ramp_end}s]:   Spike to {self.peak_users} users (spawn_rate={self.ramp_spawn_rate}/s)")
            print(f"  Phase 3 [{self.t_ramp_end}s-{self.t_hold_end}s]:  Hold at {self.peak_users} users")
            print(f"  Phase 4 [{self.t_hold_end}s-{self.t_cooldown_end}s]: Cooldown to {self.baseline_users} users")
            print(f"  Total duration: {self.t_cooldown_end}s")

        def tick(self):
            run_time = self.get_run_time()

            if run_time < self.t_baseline_end:
                return (self.baseline_users, self.spawn_rate)

            elif run_time < self.t_ramp_end:
                return (self.peak_users, self.ramp_spawn_rate)

            elif run_time < self.t_hold_end:
                return (self.peak_users, self.spawn_rate)

            elif run_time < self.t_cooldown_end:
                return (self.baseline_users, self.spawn_rate)

            else:
                return None

  shapes_realistic.py: |
    """
    Realistic load shape - gradual ramp up, peak, then cool down.

    Simulates real-world traffic patterns where load builds gradually,
    sustains at peak, and then tapers off.

    Pattern:
      1. Warm-up:   Gentle ramp to get the system going
      2. Ramp:      Increase load to target
      3. Peak:      Sustain full load
      4. Taper:     Gradually reduce
      5. Cool-down: Low load before stopping

    Configuration via environment variables:
        USERS:       Target peak users (default: 50)
        SPAWN_RATE:  Base spawn rate (default: 5)
        RUN_TIME_SECONDS: Approximate total duration in seconds (default: 300)
    """
    import os
    from locust import LoadTestShape


    class RealisticShape(LoadTestShape):
        """
        Realistic traffic pattern - warm-up -> ramp -> peak -> taper -> cool-down.
        Total duration is approximately RUN_TIME_SECONDS.
        """

        def tick(self):
            run_time = self.get_run_time()

            peak_users = int(os.environ.get("USERS", "50"))
            spawn_rate = int(os.environ.get("SPAWN_RATE", "5"))
            total_time = int(os.environ.get("RUN_TIME_SECONDS", "300"))

            # Distribute time across phases (as fractions of total)
            # 10% warm-up, 15% ramp, 40% peak, 20% taper, 15% cool-down
            t_warmup = int(total_time * 0.10)
            t_ramp = int(total_time * 0.25)    # cumulative
            t_peak = int(total_time * 0.65)    # cumulative
            t_taper = int(total_time * 0.85)   # cumulative
            t_end = total_time                 # cumulative

            warmup_users = max(2, int(peak_users * 0.1))
            ramp_users = int(peak_users * 0.5)
            taper_users = int(peak_users * 0.3)
            cooldown_users = max(2, int(peak_users * 0.05))

            if run_time < t_warmup:
                # Phase 1: Warm-up - small number of users
                return (warmup_users, max(1, spawn_rate // 2))

            elif run_time < t_ramp:
                # Phase 2: Ramp - increase to half capacity
                return (ramp_users, spawn_rate)

            elif run_time < t_peak:
                # Phase 3: Peak - full load sustained
                return (peak_users, spawn_rate)

            elif run_time < t_taper:
                # Phase 4: Taper - reduce load
                return (taper_users, spawn_rate)

            elif run_time < t_end:
                # Phase 5: Cool-down - minimal load
                return (cooldown_users, spawn_rate)

            else:
                # Test complete
                return None

  shapes_custom.py: |
    """
    Custom load shape - fully configurable via CUSTOM_STAGES environment variable.

    Reads a JSON array of stages, each with:
      - duration:   cumulative seconds from test start when this stage ends
      - users:      number of concurrent users
      - spawn_rate: users spawned per second

    Configuration via environment variables:
        CUSTOM_STAGES: JSON array of stage objects (required)

    Example CUSTOM_STAGES:
        [
            {"duration": 60,  "users": 10,  "spawn_rate": 2},
            {"duration": 120, "users": 50,  "spawn_rate": 10},
            {"duration": 240, "users": 100, "spawn_rate": 10},
            {"duration": 300, "users": 20,  "spawn_rate": 5}
        ]
    """
    import os
    import json
    from locust import LoadTestShape


    class CustomShape(LoadTestShape):
        """
        Fully configurable load shape via CUSTOM_STAGES JSON env var.
        Each stage defines a cumulative duration, user count, and spawn rate.
        """

        def __init__(self):
            super().__init__()
            stages_json = os.environ.get("CUSTOM_STAGES", "[]")
            try:
                self.stages = json.loads(stages_json)
            except json.JSONDecodeError as e:
                print(f"ERROR: Failed to parse CUSTOM_STAGES: {e}")
                print(f"  Value was: {stages_json[:200]}")
                self.stages = []

            if not self.stages:
                print("WARNING: CUSTOM_STAGES is empty or invalid. Test will stop immediately.")
            else:
                print(f"CustomShape loaded {len(self.stages)} stages:")
                for i, stage in enumerate(self.stages):
                    print(f"  Stage {i+1}: until {stage['duration']}s -> {stage['users']} users @ {stage['spawn_rate']}/s")

        def tick(self):
            run_time = self.get_run_time()

            for stage in self.stages:
                if run_time < stage["duration"]:
                    return (stage["users"], stage["spawn_rate"])

            # Past all stages - stop
            return None

  hooks_init.py: |
    # Hooks package

  hooks_mlflow.py: |
    """
    MLflow event hooks for Locust.
    Buffers metrics and sends to MLflow at test end to avoid performance impact.
    Also samples time-series metrics (user count, RPS, latency) every second
    via a background greenlet.
    
    Supports both:
    - Standard MLflow: set MLFLOW_URL
    - SageMaker MLflow: set MLFLOW_TRACKING_ARN + AWS credentials
    """
    import os
    import uuid
    import logging
    from datetime import datetime
    from collections import defaultdict
    from locust import events
    import gevent

    # Suppress noisy botocore/boto3 credential logging
    logging.getLogger("botocore").setLevel(logging.WARNING)
    logging.getLogger("boto3").setLevel(logging.WARNING)
    
    # Generate unique ID for this module instance
    _MODULE_ID = str(uuid.uuid4())[:8]
    print(f"DEBUG: mlflow_hooks module loaded, instance={_MODULE_ID}")

    # Try to import mlflow, gracefully handle if not available
    try:
        import mlflow
        MLFLOW_AVAILABLE = True
    except ImportError:
        MLFLOW_AVAILABLE = False
        print("INFO: mlflow not installed, metrics will be logged to console only")

    # Try to import sagemaker_mlflow for ARN support
    try:
        import sagemaker_mlflow
        SAGEMAKER_AVAILABLE = True
    except ImportError:
        SAGEMAKER_AVAILABLE = False

    # Buffer for metrics - avoid calling MLflow on every request
    metrics_buffer = defaultdict(list)
    _run_started = False

    # Time-series buffer: list of dicts sampled every second
    _timeseries_buffer = []
    _sampling_greenlet = None


    def _register_listeners():
        """Register event listeners only once using env var as cross-import guard."""
        # Use environment variable as flag (persists across module reimports)
        existing = os.environ.get("_MLFLOW_HOOKS_REGISTERED", "")
        print(f"DEBUG: _register_listeners called from module {_MODULE_ID}, existing={existing}")
        
        if existing:
            print(f"INFO: MLflow hooks already registered by {existing}, skipping")
            return
        os.environ["_MLFLOW_HOOKS_REGISTERED"] = _MODULE_ID
        
        events.test_start.add_listener(_on_test_start)
        events.request.add_listener(_on_request)
        events.test_stop.add_listener(_on_test_stop)
        print(f"INFO: MLflow event listeners registered by module {_MODULE_ID}")


    def _on_test_start(environment, **kwargs):
        """Called when test starts - just mark that MLflow should be used at end."""
        global _run_started
        
        # Guard against double initialization
        if _run_started:
            return
        
        # Check if MLflow is enabled
        enable_mlflow = os.environ.get("ENABLE_MLFLOW", "false").lower() == "true"
        if not enable_mlflow:
            print("INFO: MLflow disabled (ENABLE_MLFLOW != true)")
            return
        
        # Check for tracking URI
        tracking_arn = os.environ.get("MLFLOW_TRACKING_ARN", "")
        mlflow_url = os.environ.get("MLFLOW_URL", "")
        
        if not tracking_arn and not mlflow_url:
            print("INFO: Neither MLFLOW_TRACKING_ARN nor MLFLOW_URL set, skipping MLflow")
            return
            
        if not MLFLOW_AVAILABLE:
            print("INFO: mlflow package not installed")
            return
        
        # Just mark that we should log to MLflow at test end
        # DO NOT touch MLflow API here - all interaction happens in _on_test_stop
        _run_started = True
        
        # Generate run name for later
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        users = os.environ.get("USERS", "?")
        model = os.environ.get("MODEL", "unknown").split("/")[-1]
        run_name = f"locust-{model}-{users}u-{timestamp}"
        os.environ["_MLFLOW_RUN_NAME"] = run_name
        
        print(f"MLflow will log at test end, run name: {run_name}")

        # Start time-series sampling greenlet
        _start_timeseries_sampling(environment)


    def _start_timeseries_sampling(environment):
        """Spawn a background greenlet that samples metrics every second."""
        global _sampling_greenlet

        def _sample_loop():
            second = 0
            while True:
                gevent.sleep(1)
                try:
                    runner = environment.runner
                    stats = runner.stats.total
                    sample = {
                        "second": second,
                        "active_users": runner.user_count,
                        "target_users": runner.target_user_count or 0,
                        "requests_per_sec": round(stats.current_rps, 2),
                        "failures_per_sec": round(stats.current_fail_per_sec, 2),
                        "avg_response_time_ms": round(stats.avg_response_time, 2),
                        "total_requests": stats.num_requests,
                        "total_failures": stats.num_failures,
                        "fail_ratio": round(stats.fail_ratio, 4),
                    }
                    _timeseries_buffer.append(sample)
                except Exception:
                    pass  # Runner may not be ready yet
                second += 1

        _sampling_greenlet = gevent.spawn(_sample_loop)
        print("INFO: Time-series sampling started (every 1s)")


    def _on_request(request_type, name, response_time, response_length, exception, **kwargs):
        """
        Called on every request - buffer metrics (don't call MLflow here).
        """
        metrics_buffer[f"{name}_response_time_ms"].append(response_time)
        metrics_buffer[f"{name}_response_length"].append(response_length or 0)
        
        if exception:
            metrics_buffer[f"{name}_failures"].append(1)
        else:
            metrics_buffer[f"{name}_successes"].append(1)


    def _on_test_stop(environment, **kwargs):
        """Called when test stops - flush metrics to MLflow."""
        global _run_started, _sampling_greenlet
        
        # Stop the sampling greenlet
        if _sampling_greenlet is not None:
            _sampling_greenlet.kill()
            _sampling_greenlet = None
            print(f"INFO: Time-series sampling stopped. Collected {len(_timeseries_buffer)} samples.")
        
        # Always print summary to console
        print("\n" + "="*60)
        print("TEST SUMMARY")
        print("="*60)
        
        stats = environment.runner.stats.total
        print(f"Total Requests: {stats.num_requests}")
        print(f"Total Failures: {stats.num_failures}")
        print(f"Failure Rate: {stats.fail_ratio * 100:.2f}%")
        print(f"Avg Response Time: {stats.avg_response_time:.2f} ms")
        print(f"Min Response Time: {stats.min_response_time:.2f} ms")
        print(f"Max Response Time: {stats.max_response_time:.2f} ms")
        print(f"Requests/sec: {stats.total_rps:.2f}")
        print("="*60 + "\n")
        
        if not MLFLOW_AVAILABLE or not _run_started:
            return
        
        try:
            # Setup MLflow connection (exactly like the working script)
            tracking_arn = os.environ.get("MLFLOW_TRACKING_ARN", "")
            mlflow_url = os.environ.get("MLFLOW_URL", "")
            tracking_uri = tracking_arn if tracking_arn else mlflow_url
            
            print(f"Connecting to MLflow: {tracking_uri[:50]}...")
            mlflow.set_tracking_uri(tracking_uri)
            
            experiment_name = os.environ.get("MLFLOW_EXPERIMENT", "llamastack-benchmarks")
            mlflow.set_experiment(experiment_name)
            
            # Use context manager (like the working script)
            run_name = os.environ.get("_MLFLOW_RUN_NAME", "locust-test")
            with mlflow.start_run(run_name=run_name):
                # Log test parameters
                mlflow.log_param("users", os.environ.get("USERS", "unknown"))
                mlflow.log_param("spawn_rate", os.environ.get("SPAWN_RATE", "unknown"))
                mlflow.log_param("run_time_seconds", os.environ.get("RUN_TIME_SECONDS", "unknown"))
                mlflow.log_param("host", environment.host or "unknown")
                mlflow.log_param("mcp_server", os.environ.get("MCP_SERVER", "unknown"))
                mlflow.log_param("model", os.environ.get("MODEL", "unknown"))
                mlflow.log_param("load_shape", os.environ.get("LOAD_SHAPE", "steady"))
                
                # Log EXTRA_ENV params (shape-specific like SPIKE_*, HPA_*, etc.)
                extra_env = os.environ.get("EXTRA_ENV", "")
                if extra_env:
                    for line in extra_env.strip().splitlines():
                        line = line.strip()
                        if "=" in line:
                            key, value = line.split("=", 1)
                            mlflow.log_param(key.lower(), value)
                
                # Log CUSTOM_STAGES if present
                custom_stages = os.environ.get("CUSTOM_STAGES", "[]")
                if custom_stages and custom_stages != "[]":
                    mlflow.log_param("custom_stages", custom_stages)
                
                # Log summary metrics (single values)
                all_response_times = []
                total_successes = 0
                total_failures_count = 0
                for name, values in metrics_buffer.items():
                    if not values:
                        continue
                    if "response_time" in name:
                        all_response_times.extend(values)
                    elif "successes" in name:
                        total_successes += sum(values)
                    elif "failures" in name:
                        total_failures_count += sum(values)
                
                if all_response_times:
                    sorted_rt = sorted(all_response_times)
                    mlflow.log_metric("response_time_avg_ms", sum(sorted_rt) / len(sorted_rt))
                    mlflow.log_metric("response_time_min_ms", sorted_rt[0])
                    mlflow.log_metric("response_time_max_ms", sorted_rt[-1])
                    mlflow.log_metric("response_time_p50_ms", sorted_rt[len(sorted_rt)//2])
                    mlflow.log_metric("response_time_p95_ms", sorted_rt[int(len(sorted_rt)*0.95)])
                    mlflow.log_metric("response_time_p99_ms", sorted_rt[int(len(sorted_rt)*0.99)])
                
                mlflow.log_metric("total_requests", stats.num_requests)
                mlflow.log_metric("total_failures", stats.num_failures)
                mlflow.log_metric("failure_rate_pct", stats.fail_ratio * 100)
                mlflow.log_metric("requests_per_second", stats.total_rps)
                
                # Log time-series metrics in batches (up to 1000 per API call)
                if _timeseries_buffer:
                    print(f"Logging {len(_timeseries_buffer)} time-series samples to MLflow (batched)...")
                    from mlflow.entities import Metric
                    client = mlflow.tracking.MlflowClient()
                    run_id = mlflow.active_run().info.run_id
                    
                    ts_metrics = []
                    timestamp_ms = int(datetime.now().timestamp() * 1000)
                    for sample in _timeseries_buffer:
                        step = sample["second"]
                        ts_metrics.extend([
                        Metric("active_users", sample["active_users"], timestamp_ms, step),
                        Metric("target_users", sample["target_users"], timestamp_ms, step),
                        Metric("rps_10s_window", sample["requests_per_sec"], timestamp_ms, step),
                        Metric("failures_per_sec_10s_window", sample["failures_per_sec"], timestamp_ms, step),
                        Metric("avg_response_time_cumulative_ms", sample["avg_response_time_ms"], timestamp_ms, step),
                        Metric("total_requests_cumulative", sample["total_requests"], timestamp_ms, step),
                        Metric("total_failures_cumulative", sample["total_failures"], timestamp_ms, step),
                        Metric("fail_ratio_cumulative_pct", sample["fail_ratio"] * 100, timestamp_ms, step),
                        ])
                    
                    # log_batch accepts up to 1000 metrics per call
                    batch_size = 1000
                    for i in range(0, len(ts_metrics), batch_size):
                        batch = ts_metrics[i:i + batch_size]
                        client.log_batch(run_id, metrics=batch)
                    
                    print(f"Time-series logging complete ({len(ts_metrics)} metrics in {((len(ts_metrics)-1)//batch_size)+1} batch(es)).")
            
            print("MLflow run completed successfully")
            
        except Exception as e:
            print(f"WARNING: Failed to log to MLflow: {e}")


    # Register listeners once when module is imported
    _register_listeners()
