apiVersion: v1
kind: ConfigMap
metadata:
  name: locustfiles
  namespace: tekton-llamastack
data:
  locustfile_main.py: |
    """
    Main entry point for Locust tests.
    All configuration is done via environment variables.

    Usage:
        locust -f locustfile_main.py --host http://llamastack:8321 --headless
        
    Environment Variables:
        USERS              - Number of concurrent users (default: 10)
        SPAWN_RATE         - Users to spawn per second (default: 1)  
        RUN_TIME_SECONDS   - Test duration in seconds (default: 60)
        MCP_SERVER         - MCP server URL (default: in-cluster)
        MODEL              - Model name (default: vllm-inference/llama-32-3b-instruct)
        PROMPT             - Test prompt (default: "What is Kubernetes?")
        LOAD_SHAPE         - Shape to use: steady (default: steady)
        MLFLOW_URL         - MLflow tracking server URL (optional)
        MLFLOW_EXPERIMENT  - MLflow experiment name (default: llamastack-benchmarks)
    """
    import os
    import sys

    # Add current directory to path for imports
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, current_dir)

    # Import user classes - Locust auto-discovers HttpUser subclasses
    from locustfile_users import ResponsesMCPUser

    # Import hooks - registers event listeners as side effect
    from hooks import mlflow_hooks

    # Import shape based on LOAD_SHAPE env var
    shape_name = os.environ.get("LOAD_SHAPE", "steady")

    if shape_name == "steady":
        from shapes.steady import SteadyShape
    else:
        print(f"WARNING: Unknown shape '{shape_name}', using steady")
        from shapes.steady import SteadyShape

  locustfile_users.py: |
    """
    User classes for LlamaStack performance testing.
    Each user class represents a different type of API consumer.
    """
    import os
    import json
    from locust import HttpUser, task, between


    class ResponsesMCPUser(HttpUser):
        """
        User that calls the Responses API with MCP tool calling.
        This simulates a client using LlamaStack's agentic capabilities.
        """
        wait_time = between(1, 3)
        
        def on_start(self):
            """Called when a user starts - setup configuration."""
            self.mcp_server = os.environ.get("MCP_SERVER", "http://sdg-docs-mcp-server.llamastack.svc.cluster.local:8000/sse")
            self.model = os.environ.get("MODEL", "vllm-inference/llama-32-3b-instruct")
            self.prompt = os.environ.get("PROMPT", "What is Kubernetes?")
            
        @task
        def call_responses_with_mcp(self):
            """Call Responses API with MCP tool."""
            payload = {
                "model": self.model,
                "input": self.prompt,
                "tools": [{
                    "type": "mcp",
                    "server_label": "deepwiki",
                    "server_url": self.mcp_server,
                    "require_approval": "never"
                }]
            }
            
            with self.client.post(
                "/v1/responses",
                json=payload,
                name="responses-mcp",
                catch_response=True
            ) as response:
                if response.status_code == 200:
                    try:
                        data = response.json()
                        if "output" in data or "choices" in data:
                            response.success()
                        else:
                            response.failure(f"Unexpected response format: {list(data.keys())}")
                    except json.JSONDecodeError:
                        response.failure("Invalid JSON response")
                else:
                    response.failure(f"HTTP {response.status_code}: {response.text[:200]}")

  shapes_init.py: |
    # Load shapes package

  shapes_steady.py: |
    """
    Steady load shape - maintains constant user count.
    """
    import os
    from locust import LoadTestShape


    class SteadyShape(LoadTestShape):
        """
        Constant load shape - maintains steady user count for duration.
        
        Configuration via environment variables:
            USERS: Number of concurrent users (default: 10)
            SPAWN_RATE: Users to spawn per second (default: 1)
            RUN_TIME_SECONDS: Test duration in seconds (default: 60)
        """
        
        def tick(self):
            """
            Called ~1/second. Return (users, spawn_rate) or None to stop.
            """
            users = int(os.environ.get("USERS", "10"))
            spawn_rate = int(os.environ.get("SPAWN_RATE", "1"))
            run_time = int(os.environ.get("RUN_TIME_SECONDS", "60"))
            
            run_time_elapsed = self.get_run_time()
            
            if run_time_elapsed > run_time:
                return None  # Stop the test
                
            return (users, spawn_rate)

  hooks_init.py: |
    # Hooks package

  hooks_mlflow.py: |
    """
    MLflow event hooks for Locust.
    Buffers metrics and sends to MLflow at test end to avoid performance impact.
    
    Supports both:
    - Standard MLflow: set MLFLOW_URL
    - SageMaker MLflow: set MLFLOW_TRACKING_ARN + AWS credentials
    """
    import os
    import uuid
    import traceback
    from datetime import datetime
    from collections import defaultdict
    from locust import events
    
    # Generate unique ID for this module instance
    _MODULE_ID = str(uuid.uuid4())[:8]
    print(f"DEBUG: mlflow_hooks module loaded, instance={_MODULE_ID}")

    # Try to import mlflow, gracefully handle if not available
    try:
        import mlflow
        MLFLOW_AVAILABLE = True
    except ImportError:
        MLFLOW_AVAILABLE = False
        print("INFO: mlflow not installed, metrics will be logged to console only")

    # Try to import sagemaker_mlflow for ARN support
    try:
        import sagemaker_mlflow
        SAGEMAKER_AVAILABLE = True
    except ImportError:
        SAGEMAKER_AVAILABLE = False

    # Buffer for metrics - avoid calling MLflow on every request
    metrics_buffer = defaultdict(list)
    _run_started = False


    def _register_listeners():
        """Register event listeners only once using env var as cross-import guard."""
        # Use environment variable as flag (persists across module reimports)
        existing = os.environ.get("_MLFLOW_HOOKS_REGISTERED", "")
        print(f"DEBUG: _register_listeners called from module {_MODULE_ID}, existing={existing}")
        
        if existing:
            print(f"INFO: MLflow hooks already registered by {existing}, skipping")
            return
        os.environ["_MLFLOW_HOOKS_REGISTERED"] = _MODULE_ID
        
        events.test_start.add_listener(_on_test_start)
        events.request.add_listener(_on_request)
        events.test_stop.add_listener(_on_test_stop)
        print(f"INFO: MLflow event listeners registered by module {_MODULE_ID}")


    def _on_test_start(environment, **kwargs):
        """Called when test starts - just mark that MLflow should be used at end."""
        global _run_started
        
        # Guard against double initialization
        if _run_started:
            return
        
        # Check if MLflow is enabled
        enable_mlflow = os.environ.get("ENABLE_MLFLOW", "false").lower() == "true"
        if not enable_mlflow:
            print("INFO: MLflow disabled (ENABLE_MLFLOW != true)")
            return
        
        # Check for tracking URI
        tracking_arn = os.environ.get("MLFLOW_TRACKING_ARN", "")
        mlflow_url = os.environ.get("MLFLOW_URL", "")
        
        if not tracking_arn and not mlflow_url:
            print("INFO: Neither MLFLOW_TRACKING_ARN nor MLFLOW_URL set, skipping MLflow")
            return
            
        if not MLFLOW_AVAILABLE:
            print("INFO: mlflow package not installed")
            return
        
        # Just mark that we should log to MLflow at test end
        # DO NOT touch MLflow API here - all interaction happens in _on_test_stop
        _run_started = True
        
        # Generate run name for later
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        users = os.environ.get("USERS", "?")
        model = os.environ.get("MODEL", "unknown").split("/")[-1]
        run_name = f"locust-{model}-{users}u-{timestamp}"
        os.environ["_MLFLOW_RUN_NAME"] = run_name
        
        print(f"MLflow will log at test end, run name: {run_name}")


    def _on_request(request_type, name, response_time, response_length, exception, **kwargs):
        """
        Called on every request - buffer metrics (don't call MLflow here).
        """
        metrics_buffer[f"{name}_response_time_ms"].append(response_time)
        metrics_buffer[f"{name}_response_length"].append(response_length or 0)
        
        if exception:
            metrics_buffer[f"{name}_failures"].append(1)
        else:
            metrics_buffer[f"{name}_successes"].append(1)


    def _on_test_stop(environment, **kwargs):
        """Called when test stops - flush metrics to MLflow."""
        global _run_started
        
        # Always print summary to console
        print("\n" + "="*60)
        print("TEST SUMMARY")
        print("="*60)
        
        stats = environment.runner.stats.total
        print(f"Total Requests: {stats.num_requests}")
        print(f"Total Failures: {stats.num_failures}")
        print(f"Failure Rate: {stats.fail_ratio * 100:.2f}%")
        print(f"Avg Response Time: {stats.avg_response_time:.2f} ms")
        print(f"Min Response Time: {stats.min_response_time:.2f} ms")
        print(f"Max Response Time: {stats.max_response_time:.2f} ms")
        print(f"Requests/sec: {stats.total_rps:.2f}")
        print("="*60 + "\n")
        
        if not MLFLOW_AVAILABLE or not _run_started:
            return
        
        try:
            # Setup MLflow connection (exactly like the working script)
            tracking_arn = os.environ.get("MLFLOW_TRACKING_ARN", "")
            mlflow_url = os.environ.get("MLFLOW_URL", "")
            tracking_uri = tracking_arn if tracking_arn else mlflow_url
            
            print(f"Connecting to MLflow: {tracking_uri[:50]}...")
            mlflow.set_tracking_uri(tracking_uri)
            
            experiment_name = os.environ.get("MLFLOW_EXPERIMENT", "llamastack-benchmarks")
            mlflow.set_experiment(experiment_name)
            
            # Use context manager (like the working script)
            run_name = os.environ.get("_MLFLOW_RUN_NAME", "locust-test")
            with mlflow.start_run(run_name=run_name):
                # Log test parameters
                mlflow.log_param("users", os.environ.get("USERS", "unknown"))
                mlflow.log_param("spawn_rate", os.environ.get("SPAWN_RATE", "unknown"))
                mlflow.log_param("run_time_seconds", os.environ.get("RUN_TIME_SECONDS", "unknown"))
                mlflow.log_param("host", environment.host or "unknown")
                mlflow.log_param("mcp_server", os.environ.get("MCP_SERVER", "unknown"))
                mlflow.log_param("model", os.environ.get("MODEL", "unknown"))
                mlflow.log_param("load_shape", os.environ.get("LOAD_SHAPE", "steady"))
                
                # Log aggregated metrics from buffer
                for name, values in metrics_buffer.items():
                    if not values:
                        continue
                        
                    if "response_time" in name:
                        mlflow.log_metric(f"{name}_avg", sum(values) / len(values))
                        mlflow.log_metric(f"{name}_min", min(values))
                        mlflow.log_metric(f"{name}_max", max(values))
                        mlflow.log_metric(f"{name}_p50", sorted(values)[len(values)//2])
                        mlflow.log_metric(f"{name}_p95", sorted(values)[int(len(values)*0.95)])
                    elif "successes" in name or "failures" in name:
                        mlflow.log_metric(name, sum(values))
                
                # Log final aggregated stats
                mlflow.log_metric("total_requests", stats.num_requests)
                mlflow.log_metric("total_failures", stats.num_failures)
                mlflow.log_metric("failure_rate", stats.fail_ratio)
                mlflow.log_metric("avg_response_time_ms", stats.avg_response_time)
                mlflow.log_metric("min_response_time_ms", stats.min_response_time)
                mlflow.log_metric("max_response_time_ms", stats.max_response_time)
                mlflow.log_metric("requests_per_second", stats.total_rps)
            
            print("MLflow run completed successfully")
            
        except Exception as e:
            print(f"WARNING: Failed to log to MLflow: {e}")


    # Register listeners once when module is imported
    _register_listeners()
