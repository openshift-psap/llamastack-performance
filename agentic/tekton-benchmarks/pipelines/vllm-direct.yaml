apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: vllm-direct-benchmark
  namespace: tekton-llamastack
spec:
  description: |
    Benchmark vLLM directly (no LlamaStack).
    Deploys vLLM, runs Locust ChatCompletions test, then cleans up.
  params:
    - name: NAMESPACE
      description: Namespace for deployment
      type: string
      default: "llamastack-bench"
    - name: MODEL_NAME
      description: vLLM model/InferenceService name
      type: string
      default: "llama-32-3b-instruct"
    # Test params
    - name: USERS
      description: Number of concurrent users
      type: string
      default: "10"
    - name: SPAWN_RATE
      description: Users per second
      type: string
      default: "1"
    - name: RUN_TIME_SECONDS
      description: Test duration
      type: string
      default: "60"
    - name: MODEL
      description: Model name for vLLM (no provider prefix)
      type: string
      default: "llama-32-3b-instruct"
    - name: PROMPT
      description: Test prompt
      type: string
      default: "What is the capital of France?"
    - name: LOAD_SHAPE
      description: "Load shape: steady | spike | realistic | custom"
      type: string
      default: "steady"
    - name: CUSTOM_STAGES
      description: JSON array of stages for custom load shape
      type: string
      default: "[]"
    - name: EXTRA_ENV
      description: "Additional env vars for shapes (KEY=VALUE per line)"
      type: string
      default: ""
    # MLflow params
    - name: ENABLE_MLFLOW
      description: Enable MLflow logging (true/false)
      type: string
      default: "false"
    - name: MLFLOW_URL
      description: MLflow tracking server URL (optional)
      type: string
      default: ""
    - name: MLFLOW_EXPERIMENT
      description: MLflow experiment name
      type: string
      default: "llamastack-benchmarks"
    # Control
    - name: SKIP_CLEANUP
      description: Skip cleanup for debugging
      type: string
      default: "false"
  tasks:
    - name: generate-configmap
      taskRef:
        name: generate-configmap

    - name: deploy-vllm
      taskRef:
        name: deploy-vllm
      params:
        - name: NAMESPACE
          value: $(params.NAMESPACE)
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)

    - name: run-locust
      taskRef:
        name: run-locust
      runAfter:
        - deploy-vllm
        - generate-configmap
      params:
        - name: HOST
          value: "http://$(params.MODEL_NAME)-predictor.$(params.NAMESPACE).svc.cluster.local:80"
        - name: USER_CLASS
          value: "ChatCompletionsUser"
        - name: USERS
          value: $(params.USERS)
        - name: SPAWN_RATE
          value: $(params.SPAWN_RATE)
        - name: RUN_TIME_SECONDS
          value: $(params.RUN_TIME_SECONDS)
        - name: MODEL
          value: $(params.MODEL)
        - name: PROMPT
          value: $(params.PROMPT)
        - name: LOAD_SHAPE
          value: $(params.LOAD_SHAPE)
        - name: CUSTOM_STAGES
          value: $(params.CUSTOM_STAGES)
        - name: EXTRA_ENV
          value: $(params.EXTRA_ENV)
        - name: ENABLE_MLFLOW
          value: $(params.ENABLE_MLFLOW)
        - name: MLFLOW_URL
          value: $(params.MLFLOW_URL)
        - name: MLFLOW_EXPERIMENT
          value: $(params.MLFLOW_EXPERIMENT)

    - name: cleanup
      taskRef:
        name: cleanup
      runAfter:
        - run-locust
      params:
        - name: NAMESPACE
          value: $(params.NAMESPACE)
        - name: MODEL_NAME
          value: $(params.MODEL_NAME)
        - name: SKIP_CLEANUP
          value: $(params.SKIP_CLEANUP)
        - name: CLEANUP_VLLM
          value: "true"
        - name: CLEANUP_MCP
          value: "false"
